{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4159be0e",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 12) Newton methods\n",
    "\n",
    "## Last time\n",
    "\n",
    "* Convergence classes\n",
    "* Intro to Newton methods\n",
    "\n",
    "## Today\n",
    "\n",
    " 1. Newton-Raphson Method  \n",
    "   1.1 An implementation\n",
    " 2. Convergence of fixed-point (by mean value theorem)  \n",
    " 3. Convergence of fixed-point (by Taylor series)\n",
    " 4.  A fresh derivation of Newton's method via convergence theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb781a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "default(linewidth=4, legendfontsize=12)\n",
    "\n",
    "f(x) = cos(x) - x\n",
    "hasroot(f, a, b) = f(a) * f(b) < 0\n",
    "function bisect_iter(f, a, b, tol)\n",
    "    hist = Float64[]\n",
    "    while abs(b - a) > tol\n",
    "        mid = (a + b) / 2\n",
    "        push!(hist, mid)\n",
    "        if hasroot(f, a, mid)\n",
    "            b = mid\n",
    "        else\n",
    "            a = mid\n",
    "        end\n",
    "    end\n",
    "    hist\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0c695",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Convergence classes\n",
    "\n",
    ":::{tip}\n",
    "Check this [Reading](https://en.wikipedia.org/wiki/Rate_of_convergence)\n",
    ":::\n",
    "\n",
    "A convergent rootfinding algorithm produces a sequence of approximations $x_k$ such that $$\\lim_{k \\to \\infty} x_k \\to x_*$$ where $f(x_*) = 0$.  For analysis, it is convenient to define the errors $e_k = x_k - x_*$. We say that an iterative algorithm is **$q$-linearly convergent** if $$\\lim_{k \\to \\infty} |e_{k+1}| / |e_k| = \\rho < 1.$$  (The $q$ is for \"quotient\".)  A smaller convergence factor $\\rho$ represents faster convergence.  A slightly weaker condition ($r$-linear convergence or just **linear convergence** - the \"r\" here is for \"root\") is that\n",
    "$$ |e_k| \\le \\epsilon_k $$\n",
    "for all sufficiently large $k$ when the sequence $\\{\\epsilon_k\\}$ converges $q$-linearly to 0.\n",
    "\n",
    "We saw that:\n",
    "\n",
    ":::{note}\n",
    "- Why is $r$-convergence considered a weaker form of convergence relative to $q$-convergence? Notice that root convergence is concerned only with the overall rate of decrease of the error while quotient convergence requires the error to decrease at _each iteration_ of the algorithm. Thus $q$-convergence is a _stronger_ form of convergence than $r$-convergence \n",
    "- $q$-convergence implies ($\\implies$) $r$-convergence.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300477cf",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "hist = bisect_iter(f, -1, 3, 1e-10)\n",
    "r = hist[end] # What are we trusting?\n",
    "hist = hist[1:end-1]\n",
    "scatter( abs.(hist .- r), yscale=:log10)\n",
    "ks = 1:length(hist)\n",
    "ρ = 0.5\n",
    "plot!(ks, 4 * (ρ .^ ks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87ee9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Newton-Raphson Method\n",
    "\n",
    "Much of numerical analysis reduces to [Taylor series](https://en.wikipedia.org/wiki/Taylor_series), the approximation\n",
    "$$ f(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x - x_0)^2 / 2 + \\underbrace{\\dotsb}_{O((x-x_0)^3)} $$\n",
    "centered on some reference point $x_0$.\n",
    "\n",
    "In numerical computation, it is exceedingly rare to look beyond the first-order approximation\n",
    "$$ \\tilde f_{x_0}(x) = f(x_0) + f'(x_0)(x - x_0) . $$\n",
    "Since $\\tilde f_{x_0}(x)$ is a linear function, we can explicitly compute the unique solution of $\\tilde f_{x_0}(x) = 0$ as\n",
    "$$ x = x_0 - \\frac{f(x_0)}{f'(x_0)} . $$\n",
    "This is Newton's Method (aka Newton-Raphson or Newton-Raphson-Simpson) for finding the roots of differentiable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc047b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 An implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8b234",
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "function newton(f, fp, x0; tol=1e-8, verbose=false)\n",
    "    x = x0\n",
    "    for k in 1:100 # max number of iterations\n",
    "        fx = f(x)\n",
    "        fpx = fp(x)\n",
    "        if verbose\n",
    "            println(\"iteration $k: x=$x,  f(x)=$fx,  f'(x)=$fpx\")\n",
    "        end\n",
    "        if abs(fx) < tol\n",
    "            return x, fx, k\n",
    "        end\n",
    "        x = x - fx / fpx\n",
    "    end\n",
    "end\n",
    "\n",
    "f(x) = cos(x) - x\n",
    "fp(x) = -sin(x) - 1\n",
    "newton(f, fp, 1; tol=1e-15, verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131d864",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### That's really fast!\n",
    "\n",
    "* 10 digits of accuracy in 4 iterations.\n",
    "* How is this convergence test different from the one we used for bisection?\n",
    "* How can this break down?\n",
    "\n",
    "$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f32a67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "newton(f, fp, -pi/2+.1; verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f62a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Convergence of fixed-point (by mean value theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd43ed",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Consider the iteration\n",
    "$$x_{k+1} = g(x_k)$$\n",
    "where $g$ is a continuously differentiable function.\n",
    "Suppose that there exists a fixed point $x_* = g(x_*)$.  By the [mean value theorem](https://en.wikipedia.org/wiki/Mean_value_theorem), we have that\n",
    "$$ x_{k+1} - x_* = g(x_k) - g(x_*) = g'(c_k) (x_k - x_*) $$\n",
    "for some $c_i$ between $x_k$ and $x_*$.\n",
    "\n",
    "Taking absolute values, $$|e_{k+1}| = |g'(c_k)| |e_k|,$$ which converges to zero if $|g'(c_k)| < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4816b",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/e/ee/Mvt2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c45386",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Convergence of fixed-point (by Taylor series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b341cff",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Consider the iteration\n",
    "$$x_{k+1} = g(x_k)$$\n",
    "where $g$ is a continuously differentiable function.\n",
    "Suppose that there exists a fixed point $x_* = g(x_*)$. There exists a Taylor series at $x_*$,\n",
    "$$ g(x_k) = g(x_*) + g'(x_*)(x_k - x_*) + O((x_k-x_*)^2) $$\n",
    "and thus\n",
    "\\begin{align}\n",
    "x_{k+1} - x_* &= g(x_k) - g(x_*) \\\\\n",
    "&= g'(x_*) (x_k - x_*) + O((x_k - x_*)^2).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489e86f",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "In terms of the error $e_k = x_k - x_*$,\n",
    "$$ \\left\\lvert \\frac{e_{k+1}}{e_k} \\right\\rvert = \\lvert g'(x_*) \\rvert + O(e_k).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e8439",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Poll: \n",
    "- Is this convergence A=q-linear, B=r-linear, C=neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce948740",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Recall the definition of q-linear convergence\n",
    "$$ \\lim_{k\\to\\infty} \\left\\lvert \\frac{e_{k+1}}{e_k} \\right\\rvert = \\rho < 1. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a42b5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: \n",
    "- [Big $O$ (\"big oh\") notation](https://en.wikipedia.org/wiki/Big_O_notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff93ddc",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### Limit $n\\to\\infty$:\n",
    "\n",
    "We'd say an algorithm costs $O(n^2)$ if its running time on input of size $n$ is less than $c n^2$ for some constant $c$ and sufficiently large $n$.\n",
    "\n",
    "Sometimes we write $\\operatorname{cost}(\\texttt{algorithm}, n) = O(n^2)$ or (preferably) $\\operatorname{cost}(\\texttt{algorithm}) \\in O(n^2)$.\n",
    "\n",
    "Note that $O(\\log n) \\subset O(n) \\subset O(n\\log n) \\subset O(n^2) \\subset \\dotsb$.\n",
    "\n",
    "We say the algorithm is in $\\Theta(n^2)$ (\"big theta\") if\n",
    "$$ c_1 n^2 < \\operatorname{cost}(\\texttt{algorithm}) < c_2 n^2 $$\n",
    "for some positive constants $c_1,c_2$ and sufficiently large $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88413003",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Limit $h \\to 0$:\n",
    "\n",
    "In numerical analysis, we often have a small real number, and now the definitions take the limit as the small number goes to zero. So we say a term in an expression is in $O(h^2)$ if\n",
    "\n",
    "$$ \\lim_{h\\to 0} \\frac{\\operatorname{term}(h)}{h^2} < \\infty . $$\n",
    "\n",
    "Big $O$ terms can be manipulated as\n",
    "\n",
    "\\begin{align}\n",
    "h O(h^k) &= O(h^{k+1}) \\\\\n",
    "O(h^k)/h &= O(h^{k-1}) \\\\\n",
    "c O(h^k) &= O(h^k) \\\\\n",
    "O(h^k) - O(h^k) &= ?\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfdc94f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of a fixed point iteration\n",
    "\n",
    "We wanted to solve $\\cos x - x = 0$, which occurs when $g(x) = \\cos x$ is a fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafaa6b",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "xstar, _ = newton(f, fp, 1.)\n",
    "g(x) = cos(x)\n",
    "gp(x) = -sin(x)\n",
    "@show xstar\n",
    "@show gp(xstar)\n",
    "plot([x->x, g], xlims=(-2, 3), label = [\"y=x\" \"y=g(x)\"])\n",
    "scatter!([xstar], [xstar],\n",
    "    label=\"\\$x_*\\$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c73e83",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "function fixed_point(g, x, n)\n",
    "    xs = [x]\n",
    "    for k in 1:n\n",
    "        x = g(x)\n",
    "        append!(xs, x)\n",
    "    end\n",
    "    xs\n",
    "end\n",
    "\n",
    "xs = fixed_point(g, 2., 15)\n",
    "plot!(xs, g.(xs), seriestype=:path, marker=:auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eda74e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Verifying fixed point convergence theory\n",
    "\n",
    "\n",
    "$$ \\left\\lvert \\frac{e_{k+1}}{e_k} \\right\\rvert \\to \\lvert g'(x_*) \\rvert $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29642e",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "@show gp(xstar)\n",
    "es = xs .- xstar\n",
    "es[2:end] ./ es[1:end-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353d06d",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "scatter(abs.(es), yscale=:log10, label=\"fixed point\")\n",
    "plot!(k -> abs(gp(xstar))^k, label=\"\\$|g'|^k\\$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265cad05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plotting Newton convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b8eff",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "function newton_hist(f, fp, x0; tol=1e-12)\n",
    "    x = x0\n",
    "    hist = []\n",
    "    for k in 1:100 # max number of iterations\n",
    "        fx = f(x)\n",
    "        fpx = fp(x)\n",
    "        push!(hist, [x fx fpx])\n",
    "        if abs(fx) < tol\n",
    "            return vcat(hist...) # concatenate arrays\n",
    "        end\n",
    "        x = x - fx / fpx\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9af8b",
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs = newton_hist(f, fp, 1.97)\n",
    "@show x_star = xs[end,1]\n",
    "@show xs[1:end-1,1] .- x_star\n",
    "plot(xs[1:end-1,1] .- x_star, yscale=:log10, marker=:auto, xlabel = \"# of iterations\", ylabel = \"abs error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51533606",
   "metadata": {},
   "source": [
    "#### Poll: \n",
    "- Is this convergence A=q-linear, B=r-linear, C=neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a040d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Formulations are not unique (constants):\n",
    "\n",
    "If $x = g(x)$ then\n",
    "$$x = \\underbrace{x + c(g(x) - x)}_{g_2}$$\n",
    "for any constant $c \\ne 0$. Can we choose $c$ to make $\\lvert g_2'(x_*) \\rvert$ small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9472b",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "c = .5\n",
    "g2(x) = x + c * (cos(x) - x)\n",
    "g2p(x) = 1 + c * (-sin(x) - 1)\n",
    "@show g2p(xstar)\n",
    "plot([x->x, g, g2], ylims=(-5, 5), label=[\"y=x\" \"y=g(x)\" \"y=g2(x)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee64b2",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs = fixed_point(g2, 1., 15)\n",
    "xs .- xstar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6fde1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Formulations are not unique (functions)\n",
    "\n",
    "If $x = g(x)$ then\n",
    "$$x = \\underbrace{x + h(x) \\big(g(x) - x\\big)}_{g_3(x)}$$\n",
    "for any smooth $h(x) \\ne 0$. Can we choose $h(x)$ to make $\\lvert g_3'(x) \\rvert$ small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd20aa4",
   "metadata": {
    "cell_style": "split",
    "scrolled": false,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "h(x) = -1 / (gp(x) - 1)\n",
    "g3(x) = x + h(x) * (g(x) - x)\n",
    "plot([x-> x, cos, g2, g3], ylims=(-5, 5), label=[\"y=x\" \"y=g(x)\" \"y=g2(x)\" \"y=g3(x)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9909bb8",
   "metadata": {},
   "source": [
    "### Exercise 12.1:\n",
    "Compute $|g_3'(x_*)|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ca4b8",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Note:\n",
    "\n",
    "* We don't know $g'(x_*)$ in advance because we don't know $x_*$ yet.\n",
    "* This method converges very fast\n",
    "* We actually just derived Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff62d33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. A fresh derivation of Newton's method via convergence theory\n",
    "\n",
    "* A rootfinding problem $f(x) = 0$ can be converted to a fixed point problem $$x = x + f(x) =: g(x)$$ but there is no guarantee that $g'(x_*) = 1 + f'(x_*)$ will have magnitude less than 1.\n",
    "* Problem-specific algebraic manipulation can be used to make $|g'(x_*)|$ small.\n",
    "* $x = x + h(x) f(x)$ is also a valid formulation for any $h(x)$ bounded away from $0$.\n",
    "* Can we choose $h(x)$ such that $$ g'(x) = 1 + h'(x) f(x) + h(x) f'(x) = 0$$ when $f(x) = 0$?\n",
    "\n",
    "In other words,\n",
    "$$ x_{k+1} = x_k + \\underbrace{\\frac{-1}{f'(x_k)}}_{h(x_k)} f(x_k)  . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f835bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quadratic convergence!\n",
    "\n",
    "$$ \\left\\lvert \\frac{e_{k+1}}{e_k} \\right\\rvert \\to \\lvert g'(x_*) \\rvert $$\n",
    "\n",
    "* What does it mean that $g'(x_*) = 0$?\n",
    "* It turns out that Newton's method has _locally quadratic_ convergence to simple roots,\n",
    "$$\\lim_{k \\to \\infty} \\frac{|e_{k+1}|}{|e_k|^2} < \\infty.$$\n",
    "* \"The number of correct digits doubles each iteration.\"\n",
    "* Now that we know how to make a good guess accurate, the effort lies in getting a good starting guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350da54a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rootfinding summary and outlook\n",
    "\n",
    "* Newton methods are immensely successful\n",
    "  * Convergence theory is local; we need good initial guesses\n",
    "  * Computing the derivative $f'(x)$ is *intrusive* (and can be error-prone)\n",
    "    * Avoided by secant methods (approximate the derivative)\n",
    "    * Algorithmic or numerical differentiation can also be used instead of computing the derivative by hand\n",
    "  * Bisection is robust when conditions are met\n",
    "  * When does Newton diverge?\n",
    "\n",
    "* More topics\n",
    "  * Do we find *all* the roots?\n",
    "  * Times when Newton converges slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b70944",
   "metadata": {},
   "source": [
    "### Exercise 12.2:\n",
    " 1. Solve the fixed point iteration $g(x_n) = (2+{(x_n - 2)}^2)$, up to a tolerance $10^{-6}$ or maximum number of iterations $N=50$, with \\textbf{(a)} $x_0 = 2.8$,  \\textbf{(b)} $x_0 = 3.1$. Which execution is better? And why?  \n",
    " 2. Solve the fixed point iteration $g(x_n) = (2+{(x_n - 2)}^3)$, up to a tolerance $10^{-6}$ or maximum number of iterations $N=50$, with \\textbf{(a)} $x_0 = 2.8$,  \\textbf{(b)} $x_0 = 3.1$. Which execution is better? And why?  \n",
    " 3. Use Newton's method to solve Exercise 10.2 points 1 and 3 from the Bisection method class. Which method performs better on these functions and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e23b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "cd1892c1e2064686902fbdf399e6e3e3",
   "lastKernelId": "6eae1736-6531-4224-a6c2-38e7ff163d19"
  },
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
