{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20) Matrix condition number and SVD\n",
    "\n",
    "## Last time\n",
    "- Solving Systems\n",
    "- Review  \n",
    "\n",
    "## Today\n",
    "1. Condition Number of a Matrix  \n",
    "2. Least squares and normal equations\n",
    "3. Intro to SVD\n",
    "4. Geometry of the Singular Value Decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "default(linewidth=4, legendfontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Condition Number of a Matrix\n",
    "\n",
    "We may have informally referred to a matrix as \"ill-conditioned\" when the columns are nearly linearly dependent, but let's make this concept for precise.  Recall the definition of (relative) condition number:\n",
    "\n",
    "$$ \\kappa = \\max_{\\delta x} \\frac{|\\delta f| / |f|}{|\\delta x| / |x|} . $$\n",
    "\n",
    "We understood this definition for scalar problems, but it also makes sense when the inputs and/or outputs are vectors (or matrices, etc.) and absolute value is replaced by vector (or matrix) norms.  Consider matrix-vector multiplication, for which $f(x) = A x$.\n",
    "\n",
    "\n",
    "\n",
    "$$ \\kappa(A) = \\max_{\\delta x} \\frac{\\lVert A (x+\\delta x) - A x \\rVert/\\lVert A x \\rVert}{\\lVert \\delta x\\rVert/\\lVert x \\rVert}\n",
    "= \\max_{\\delta x} \\frac{\\lVert A \\delta x \\rVert}{\\lVert \\delta x \\rVert} \\, \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} = \\lVert A \\rVert \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} . $$\n",
    "\n",
    "There are two problems here:\n",
    "\n",
    "* I wrote $\\kappa(A)$ but my formula depends on $x$.\n",
    "* What is that $\\lVert A \\rVert$ beastie?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix norms induced by vector norms\n",
    "\n",
    "Vector norms are built into the linear space (and defined in term of the inner product).  Matrix norms are *induced* by vector norms, according to\n",
    "\n",
    "$$ \\lVert A \\rVert = \\max_{x \\ne 0} \\frac{\\lVert A x \\rVert}{\\lVert x \\rVert} . $$\n",
    "\n",
    "* This equation makes sense for non-square matrices -- the vector norms of the input and output spaces may differ.\n",
    "* Due to linearity, all that matters is the direction of $x$, so it could equivalently be written as\n",
    "\n",
    "$$ \\lVert A \\rVert = \\max_{\\lVert x \\rVert = 1} \\lVert A x \\rVert . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The formula makes sense, but still depends on $x$.\n",
    "\n",
    "$$\\kappa(A) = \\lVert A \\rVert \\frac{\\lVert x \\rVert}{\\lVert Ax \\rVert}$$\n",
    "\n",
    "Consider the matrix\n",
    "\n",
    "$$ A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} . $$\n",
    "\n",
    "* What is the norm of this matrix?\n",
    "* What is the condition number when $x = [1,0]^T$?\n",
    "* What is the condition number when $x = [0,1]^T$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition number of the matrix\n",
    "\n",
    "The condition number of matrix-vector multiplication depends on the vector.  The condition number of the matrix is the worst case (maximum) of the condition number for any vector, i.e.,\n",
    "\n",
    "$$ \\kappa(A) = \\max_{x \\ne 0} \\lVert A \\rVert \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} .$$\n",
    "\n",
    "If $A$ is _invertible_, then we can rephrase as\n",
    "\n",
    "$$ \\kappa(A) = \\max_{x \\ne 0} \\lVert A \\rVert \\frac{\\lVert A^{-1} (A x) \\rVert}{\\lVert A x \\rVert} =\n",
    "\\max_{A x \\ne 0} \\lVert A \\rVert \\frac{\\lVert A^{-1} (A x) \\rVert}{\\lVert A x \\rVert} = \\lVert A \\rVert \\lVert A^{-1} \\rVert . $$\n",
    "\n",
    "Evidently multiplying by a matrix is just as ill-conditioned of an operation as solving a linear system using that matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Least squares and the normal equations\n",
    "\n",
    "A **least squares problem** takes the form: given an $m\\times n$ matrix $A$ ($m \\ge n$), find $x$ such that\n",
    "$$ \\lVert Ax - b \\rVert $$\n",
    "is minimized.  If $A$ is square and full rank, then this minimizer will satisfy $A x - b = 0$, but that is not the case in general because $b$ is not in the range of $A$.\n",
    "The residual $A x - b$ must be orthogonal to the range of $A$.\n",
    "\n",
    "\n",
    "So if $b$ is in the range of $A$, we can solve $A x = b$.  If not, we need only to *orthogonally* project $b$ into the range of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution by QR (Householder triangulation)\n",
    "\n",
    "Solves $R x = Q^T b$.\n",
    "\n",
    "This method is stable and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution by Cholesky\n",
    "\n",
    "The mathematically equivalent form $(A^T A) x = A^T b$ are called the **normal equations**.  The solution process involves factoring the symmetric and positive definite $n\\times n$ matrix $A^T A$.\n",
    "\n",
    "\n",
    "The product $A^T A$ is ill-conditioned: $\\kappa(A^T A) = \\kappa(A)^2$ and can reduce the accuracy of a least squares solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution by Singular Value Decomposition\n",
    "\n",
    "Next, we will discuss a factorization\n",
    "$$ U \\Sigma V^T = A $$\n",
    "where $U$ and $V$ have orthonormal columns and $\\Sigma$ is diagonal with nonnegative entries.\n",
    "The entries of $\\Sigma$ are called [**singular values**](https://en.wikipedia.org/wiki/Singular_value) and this decomposition is the **singular value decomposition** (SVD).\n",
    "\n",
    "**Singular values** of a linear operator (matrix $A$) are the square roots of the (necessarily non-negative) eigenvalues of the self-adjoint operator $A^{*}A$ (where $A^{*}$ denotes the [adjoint](https://en.wikipedia.org/wiki/Hermitian_adjoint) of $A$.)\n",
    " \n",
    "\n",
    "It may remind you of an eigenvalue decomposition $X \\Lambda X^{-1} = A$, but\n",
    "* the SVD exists for all matrices (including non-square and deficient matrices)\n",
    "* $U,V$ have orthogonal columns (while $X$ can be arbitrarily ill-conditioned).\n",
    "* Indeed, if a matrix is symmetric and positive definite (all positive eigenvalues), then $U=V$ and $\\Sigma = \\Lambda$.\n",
    "Computing an SVD requires a somewhat complicated iterative algorithm, but a crude estimate of the cost is $2 m n^2 + 11 n^3$.  Note that this is similar to the cost of $QR$ when $m \\gg n$, but much more expensive for square matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geometry of the Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default(aspect_ratio=:equal)\n",
    "\n",
    "function peanut()\n",
    "    theta = LinRange(0, 2*pi, 50)\n",
    "    r = 1 .+ .4*sin.(3*theta) + .6*sin.(2*theta)\n",
    "    r' .* [cos.(theta) sin.(theta)]'\n",
    "end\n",
    "\n",
    "function circle()\n",
    "    theta = LinRange(0, 2*pi, 50)\n",
    "    [cos.(theta) sin.(theta)]'\n",
    "end\n",
    "\n",
    "function Aplot(A)\n",
    "    \"Plot a transformation from X to Y, first applied to a peanut shape and then to a circle\"\n",
    "    X = peanut()\n",
    "    Y = A * X\n",
    "    p = scatter(X[1,:], X[2,:], label=\"in\")\n",
    "    scatter!(p, Y[1,:], Y[2,:], label=\"out\")\n",
    "    X = circle()\n",
    "    Y = A * X\n",
    "    q = scatter(X[1,:], X[2,:], label=\"in\")\n",
    "    scatter!(q, Y[1,:], Y[2,:], label=\"out\")\n",
    "    plot(p, q, layout=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication by a scalar: $\\alpha I$\n",
    "\n",
    "Perhaps the simplest transformation is a scalar multiple of the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplot(1.1*I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal matrices \n",
    "It is the same thing if we apply it to any diagonal matrix $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplot([2 0; 0 2]) # In this case D = 2 I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal entries can be different sizes. Example:\n",
    "\n",
    "$$ A = \\begin{bmatrix} 2 & 0 \\\\ 0 & .5 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplot([2 0; 0 .5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the circles becomes an [**ellipse**](https://en.wikipedia.org/wiki/Ellipse) that is **aligned with the coordinate axes** ($a = 2 =a_{11}$ and $b=0.5=a_{22}$ for this ellipse.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Givens Rotation (as example of orthogonal matrix)\n",
    "\n",
    "We can rotate the input using a $2\\times 2$ matrix, parametrized by $\\theta$. Its transpose rotates in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function givens(theta)\n",
    "    s = sin(theta)\n",
    "    c = cos(theta)\n",
    "    [c -s; s c]\n",
    "end\n",
    "\n",
    "G = givens(0.3)\n",
    "Aplot(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the effect of its transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplot(G')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "We've previously seen that reflectors have the form $F = I - 2 v v^T$ where $v$ is a normalized vector. Reflectors satisfy $F^T F = I$ and $F = F^T$, thus $F^2 = I$ (i.e., it is an [idempotent matrix](https://en.wikipedia.org/wiki/Idempotent_matrix))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function reflect(theta)\n",
    "    v = [cos(theta), sin(theta)]\n",
    "    I - 2 * v * v'\n",
    "end\n",
    "\n",
    "Aplot(reflect(0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "The SVD is $A = U \\Sigma V^T$ where $U$ and $V$ have orthonormal columns and $\\Sigma$ is diagonal with nonnegative entries. It exists for any matrix (non-square, singular, etc.). If we think of orthogonal matrices as reflections/rotations, this says any matrix can be represented by the sequence of operations: reflect/rotate, diagonally scale, and reflect/rotate again.\n",
    "\n",
    "Let's try a random symmetric matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = randn(2, 2)\n",
    "A += A' # make symmetric\n",
    "@show det(A) # Positive means orientation is preserved\n",
    "Aplot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = svd(A) # using the Julia built-in\n",
    "@show norm(U * diagm(S) * V' - A) # Should be zero\n",
    "Aplot(V') # Rotate/reflect in preparation for scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What visual features indicate that this is a symmetric matrix?\n",
    "* Is the orthogonal matrix a reflection or rotation?\n",
    "    * Does this change when the determinant is positive versus negative (rerun the cell above as needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the parts of the SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplot(diagm(S)) # scale along axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplot(U) # rotate/reflect back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplot(U * diagm(S) * V') # SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* The circle always maps to an ellipse\n",
    "* The $U$ and $V$ factors may reflect even when $\\det A > 0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
