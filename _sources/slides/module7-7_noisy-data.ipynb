{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4159be0e",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 31) Noisy data\n",
    "\n",
    "## Last time\n",
    "\n",
    "* Compare accuracy and conditioning of splines\n",
    "* Boundary Value Problems\n",
    "* Cost for interpolation in higher dimensions\n",
    "\n",
    "## Today\n",
    "\n",
    "1. Noisy data  \n",
    "2. Regression using polynomials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb781a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "default(linewidth=4, legendfontsize=12)\n",
    "\n",
    "function vander(x, k=nothing)\n",
    "    if isnothing(k)\n",
    "        k = length(x)\n",
    "    end\n",
    "    m = length(x)\n",
    "    V = ones(m, k)\n",
    "    for j in 2:k\n",
    "        V[:, j] = V[:, j-1] .* x\n",
    "    end\n",
    "    V\n",
    "end\n",
    "\n",
    "function vander_chebyshev(x, n=nothing)\n",
    "    if isnothing(n)\n",
    "        n = length(x) # Square by default\n",
    "    end\n",
    "    m = length(x)\n",
    "    T = ones(m, n)\n",
    "    if n > 1\n",
    "        T[:, 2] = x\n",
    "    end\n",
    "    for k in 3:n\n",
    "        #T[:, k] = x .* T[:, k-1]\n",
    "        T[:, k] = 2 * x .* T[:,k-1] - T[:, k-2]\n",
    "    end\n",
    "    T\n",
    "end\n",
    "\n",
    "function interp_nearest(x, s)\n",
    "    A = zeros(length(s), length(x))\n",
    "    for (i, t) in enumerate(s)\n",
    "        loc = nothing\n",
    "        dist = Inf\n",
    "        for (j, u) in enumerate(x)\n",
    "            if abs(t - u) < dist\n",
    "                loc = j\n",
    "                dist = abs(t - u)\n",
    "            end\n",
    "        end\n",
    "        A[i, loc] = 1\n",
    "    end\n",
    "    A\n",
    "end\n",
    "\n",
    "runge(x) = 1 / (1 + 10*x^2)\n",
    "\n",
    "CosRange(a, b, n) = (a + b)/2 .+ (b - a)/2 * cos.(LinRange(-pi, 0, n))\n",
    "\n",
    "vcond(mat, points, nmax) = [cond(mat(points(-1, 1, n))) for n in 2:nmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500c740",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Lower-degree polynomials to fit noise-free data\n",
    "\n",
    "We can fit $m$ data points using an $n < m$ dimensional space of functions. This involves solving a least squares problem for the coefficients\n",
    "\n",
    "$$ \\min_c \\lVert V c - y \\rVert $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9080e",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "function chebyshev_regress_eval(x, xx, n)\n",
    "    V = vander_chebyshev(x, n)\n",
    "    @show cond(V)\n",
    "    vander_chebyshev(xx, n) / V\n",
    "end\n",
    "\n",
    "ndata, nbasis = 50, 20\n",
    "x = LinRange(-1, 1, ndata)\n",
    "xx = LinRange(-1, 1, 500)\n",
    "C = chebyshev_regress_eval(x, xx, nbasis)\n",
    "plot(xx, [runge.(xx), C * runge.(x)])\n",
    "scatter!(x, runge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ac557",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "@show size(C)\n",
    "S = svdvals(C)\n",
    "scatter(S, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33dd0e",
   "metadata": {},
   "source": [
    "## 1. Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8331a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "runge_noisy(x, sigma) = runge.(x) + randn(size(x)) * sigma\n",
    "\n",
    "x = LinRange(-1, 1, 1000)\n",
    "y = runge_noisy(x, 0.1)\n",
    "C = chebyshev_regress_eval(x, x, 8)\n",
    "plot(x, [runge.(x), C * y])\n",
    "scatter!(x, y, markersize=2, size=(1000, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ba15c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probability distributions and simulation\n",
    "\n",
    "To interpret real data, we need a model for noise. We've used the most common and convenient choice when creating the data above; the `randn` function draws from the \"standard normal\" or \"Gaussian\" distribution,\n",
    "\n",
    "$$ p(t) = \\frac{1}{\\sqrt{2 \\pi}} e^{-t^2/2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7ec0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stdnormal(t) = exp(-t^2/2.) / sqrt(2*pi)\n",
    "n = 100\n",
    "w = randn(n)\n",
    "histogram(w, bins=40, normalize=:density, xlims=(-4, 4))\n",
    "plot!(t -> n*stdnormal(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99bb3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression with noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46321f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runge_noisy(x, sigma) = runge.(x) + randn(size(x)) * sigma\n",
    "\n",
    "x = LinRange(-1, 1, 200)\n",
    "sigma = 0.1\n",
    "C = chebyshev_regress_eval(x, x, 20)\n",
    "plot(x, runge.(x), color=:black)\n",
    "plot!(x, [C * runge_noisy(x, sigma) for n in 1:20], legend=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9e8b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias-variance tradeoff\n",
    "\n",
    "The expected error in our approximation $\\hat f(x)$ of noisy data $y = f(x) + \\epsilon$ (with $\\epsilon \\sim \\mathcal N(0, \\sigma)$), can be decomposed as\n",
    "$$ E[(\\hat f(x) - y)^2] = \\sigma^2 + \\big(\\underbrace{E[\\hat f(x)] - f(x)}_{\\text{Bias}}\\big)^2 + \\underbrace{E[\\hat f(x)^2] - E[\\hat f(x)]^2}_{\\text{Variance}} . $$\n",
    "The $\\sigma^2$ term is irreducible error (purely due to observation noise), but bias and variance can be controlled by model selection.\n",
    "More complex models are more capable of expressing the underlying function $f(x)$, thus are capable of reducing bias.  However, they are also more affected by noise, thereby increasing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf412de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Regression using polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d545a",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "function chebyshev_regress_eval(x, xx, n)\n",
    "    V = vander_chebyshev(x, n)\n",
    "    vander_chebyshev(xx, n) / V\n",
    "end\n",
    "\n",
    "runge(x) = 1 / (1 + 10*x^2)\n",
    "runge_noisy(x, sigma) = runge.(x) + randn(size(x)) * sigma\n",
    "\n",
    "x = LinRange(-1, 1, 500)\n",
    "ytrain = runge_noisy(x, 0.25)\n",
    "yfit = chebyshev_regress_eval(x, x, 5) * ytrain\n",
    "size(ytrain), size(yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a7d81",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plot(x, runge.(x), label=\"runge(x)\")\n",
    "plot!(x, yfit, label=\"yfit\")\n",
    "scatter!(x, ytrain, markersize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8563ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = runge_noisy(x, 0.25)\n",
    "@show norm(yfit - ytrain)\n",
    "@show norm(yfit - ytest);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27964182",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What happens as we increase polynomial degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b4490",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "ks = 2:4:50\n",
    "p = plot()\n",
    "function residuals(k)\n",
    "    # Fit polynomial of degree k to ytrain.\n",
    "    yfit = chebyshev_regress_eval(x, x, k) * ytrain\n",
    "    plot!(x, yfit, label=\"k=$k\")\n",
    "    [norm(yfit - ytrain) norm(yfit - ytest)]\n",
    "end\n",
    "\n",
    "res = vcat([residuals(k) for k in ks]...)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6cc1f",
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@show size(res)\n",
    "\n",
    "plot(ks, res[:,1], label=\"train\", xlabel=\"polynomial degree\", ylabel=\"residual\")\n",
    "plot!(ks, res[:,2], label=\"test\")\n",
    "plot!(ks, _ -> norm(runge.(x)-ytrain), label=\"perfect train\")\n",
    "plot!(ks, _ -> norm(runge.(x)-ytest), label=\"perfect test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24596d2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation questions\n",
    "Think about these questions, re-run the notebook, and try to formulate an answer.\n",
    "Please discuss in class with a friend.\n",
    "\n",
    "* Is \"perfect train\" (residual for the noisy sample of the zero-noise function) always greater than (or less than) \"perfect test\"?\n",
    "* Can you identify when we begin \"overfitting\" by comparing \"train\" with \"perfect train\"?  Does it happen at about the same degree each time?\n",
    "* In the real world, we don't have access to the zero-noise function, thus can't mark \"perfect train\".  By looking at just \"train\" and \"test\", can you identify (roughly) when we begin overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd5b58",
   "metadata": {},
   "source": [
    "### Bias and variance over multiple training sets\n",
    "\n",
    "What have we just done?\n",
    "\n",
    "* We took one noisy sample of a function\n",
    "* Fit polynomials of increasing degree to it\n",
    "* Computed the residual of that fit on\n",
    "  * the training data\n",
    "  * an independent \"test\" sample\n",
    "  \n",
    "### What happens if we repeat this process?\n",
    "* Scroll up and re-run above\n",
    "* We'll do it many times below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d389eeb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stacking many realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca23515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degree = 7\n",
    "Y = []\n",
    "for i in 1:50\n",
    "    yi = runge_noisy(x, 0.25)\n",
    "    push!(Y, chebyshev_regress_eval(x, x, degree) * yi)\n",
    "end\n",
    "\n",
    "Y = hcat(Y...)\n",
    "@show size(Y) # (number of points in each fit, number of fits)\n",
    "plot(x, Y, label=nothing);\n",
    "plot!(x, runge.(x), color=:black, label=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fcfe1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation\n",
    "\n",
    "* Re-run the cell above for different values of `degree`.  (Set it back to a number around 7 to 10 before moving on.)\n",
    "* Low-degree polynomials are not rich enough to capture the peak of the function.\n",
    "* As we increase degree, we are able to resolve the peak better, but see more eratic behavior near the ends of the interval.  This erratic behavior is **overfitting**, which we'll quantify as *variance*.\n",
    "* This tradeoff is fundamental: richer function spaces are more capable of approximating the functions we want, but they are more easily distracted by noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf7250",
   "metadata": {
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean over all the realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ymean = sum(Y, dims=2) / size(Y, 2)\n",
    "plot(x, Ymean, label=\"\\$ E[\\\\hat{f}(x)] \\$\")\n",
    "plot!(x, runge.(x), label=\"\\$ f(x) \\$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662085f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity: Variance over the realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db059662",
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "function variance(Y)\n",
    "    \"\"\"Compute the Variance as defined at the top of this activity\"\"\"\n",
    "    ## BEGIN SOLUTION\n",
    "\n",
    "    ## END SOLUTION\n",
    "end\n",
    "\n",
    "Yvar = variance(Y)\n",
    "\n",
    "# plot(x, Yvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef70968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the following\n",
    "#@assert size(variance(Y)) == (size(Y, 1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5862a393",
   "metadata": {},
   "source": [
    "### Conclusion: Another take on the Runge phenomenon\n",
    "\n",
    "The fact that variance blows up toward the end of our interval is a property of the approximation space (polynomials).\n",
    "\n",
    "Recall that it doesn't depend on the basis used for fitting (Chebyshev in this case); that choice only relates to stability.\n",
    "\n",
    "If we could choose an approximation space such that variance was flat across the interval $[-1, 1]$, we would be able to solve interpolation problems on equally spaced grids without numerical artifacts like the Runge phenomenon.\n",
    "\n",
    "Finding spaces of functions have flat variance and are rich enough to approximate interesting functions is \"hard\" (math speak for has no general solution).\n",
    "\n",
    "It is possible in special circumstances, such as for periodic functions, in which the Fourier basis (sine and cosine functions) can be used.\n",
    "\n",
    "In practice, we often use **regularization** to modify the least squares objective such that we can reduce variance while using function spaces rich enough to keep bias low."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
