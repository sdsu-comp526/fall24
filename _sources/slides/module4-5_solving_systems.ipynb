{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18) Solving Systems\n",
    "\n",
    "## Last time\n",
    "\n",
    "* Cholesky Decomposition\n",
    "* Profiling\n",
    "\n",
    "## Today\n",
    "\n",
    "1. Solving Systenms \n",
    "2. Direct methods\n",
    "3. Iterative methods\n",
    "4. Example to solve a Partial Differential Equation (PDEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Solving systems\n",
    "\n",
    "Suppose we wish to solve a problem $Ax=b$, where $A, b$ are known. Each row of $A$ and $b$ represents a linear equation, so the problem represents a system of linear equations. \n",
    "\n",
    "The problem $Ax=b$ is easy in two particular cases:\n",
    "\n",
    "- When $A$ is diagonal. In this case, $x_j = b_j a_{j,j}^{-1}$.\n",
    "- When $A$ is triangular (can solve by back-substitution).\n",
    "\n",
    "\n",
    "Techniques to solve linear systems fall under two main categories: **direct methods** and **iterative methods**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Direct Methods\n",
    "\n",
    "These methods are designed to solve the system _exactly_.\n",
    "\n",
    "### Gaussian elimination and LU decomposition\n",
    "\n",
    "\n",
    "- [**Gaussian elimination**](https://en.wikipedia.org/wiki/Gaussian_elimination), also known as **row reduction** is probably the most popular algorithm for solving systems of linear equations by hand. But nowadays, we rarely solve systems of linear equations by hand (especially if they exceed, say, three or four equations). \n",
    "\n",
    "- Row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Then the first part of the algorithm computes an **[LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition)**, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.\n",
    "\n",
    "- Efficiency: The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of $n$ equations for $n$ unknowns by performing row operations on the matrix until it is in reduced row echelon form, and then solving for each unknown in reverse order, requires $n(n + 1)/2$ divisions, $(2n3 + 3n2 − 5n)/6$ multiplications, and $(2n3 + 3n2 − 5n)/6$ subtractions, for a total of approximately $2n^3/3$ operations. Thus it has a arithmetic complexity of $O(n^3)$.  The cost becomes prohibitive for systems with millions of equations (which are very common nowadays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other decompositions seen so far:\n",
    "- QR factorization\n",
    "- Cholesky decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Iterative Methods\n",
    "\n",
    "Because costs become prohibitive for direct solvers for systems with millions of equations (which are very common nowadays), large systems are generally solved using iterative methods. In iterative methods, a solution is approximated via repeated iterations, rather than exactly calculated. These methods need an initial value to generate a sequence of improving approximate solutions.\n",
    "\n",
    "Among iterative methods, _stationary iterative methods_ solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the **residual**), form a \"correction equation\" for which this process is repeated until predefined termination criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic iterative methods work by splitting the matrix $A$ into\n",
    "\n",
    "$$\n",
    "A = M -N\n",
    "$$\n",
    "\n",
    "where $M$ is some \"nice\" matrix (easily invertible) related to $A$.\n",
    "\n",
    "To solve a linear system $Ax=b$, once you have the decomposition $A = M-N$, then you can solve:\n",
    "\n",
    "$$\n",
    "(M-N) x = b \\, ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "M x = Nx + b\n",
    "$$\n",
    "\n",
    "And the iterations are defined so that the new approximate solution, $x^{k+1}$, is written in terms of the old approximate solution, $x^k$, as:\n",
    "\n",
    "$$\n",
    "M x^{k+1} = Nx^k + b \\, , \\quad k \\ge 0\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a linear system  linear system $A x = b$ with exact solution $x^{*}$, we define the error by\n",
    "\n",
    "$$\n",
    "e^{k}:=  x^{*} - x^k \\, , \\quad k \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** An iterative method is called _linear_ if there exists a matrix $C \\in \\R^{n \\times n}$ such that\n",
    "\n",
    "$$\n",
    "e^{k+1} = C e^k \\, , \\quad \\forall k \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this matrix is called the _iteration matrix_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** An iterative method with a given iteration matrix $C$ is called _convergent_ if the following holds\n",
    "\n",
    "$$\n",
    "\\lim_{k\\rightarrow \\infty} C^k = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem:** \n",
    "\n",
    "An iterative method with an iteration matrix $C$ is convergent if and only if its spectral radius, $\\rho (C)$ is smaller than unity, that is:\n",
    "\n",
    "$$\n",
    "\\rho(C) < 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the [spectral radius](https://en.wikipedia.org/wiki/Spectral_radius) is defined as the maximum of the absolute values of the eigenvalues of a given matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By splitting a matrix $A$ into a \"nice\" (easily invertible) part $M$ and the difference, $N$, (i.e., $A = M -N$) we can see that for the iterates\n",
    "\n",
    "$$\n",
    "M x^{k+1} = Nx^k + b \\, , \\quad k \\ge 0\n",
    "$$\n",
    "\n",
    "the iteration matrix is given by\n",
    "\n",
    "$$\n",
    "C = I - M^{-1} A = M^{-1} N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, from the iterates we have:\n",
    "\n",
    "$$\n",
    "M x^{k+1} = Nx^k + b \n",
    "$$\n",
    "\n",
    "Let $e^k = x - x^k$, we then have\n",
    "\n",
    "$$\n",
    "M(x - x^{k+1}) = N (x - x^k) + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "M e^{k+1} = N e^k +b\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{k+1} = \\underbrace{M^{-1} N}_{C} e^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common splittings:\n",
    "Basic examples of stationary iterative methods use a splitting of $A$ such as:\n",
    "\n",
    "$$\n",
    "A = D + L + U\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "D = \\textrm{diag}(A)\n",
    "$$\n",
    "\n",
    "i.e., where $D$ is only the diagonal part of $A$, and $L$ is the strict lower triangular part of $A$. Respectively, $U$ is the strict upper triangular part of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of splittings:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**Jacobi method**](https://en.wikipedia.org/wiki/Jacobi_method)\n",
    "\n",
    "$$\n",
    "M \\equiv D = \\textrm{diag}(A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this method, the iterative solution is then found as\n",
    "\n",
    "$$\n",
    "x^{k+1} = D^{-1} \\left\\{ b - (L+U) x^k \\right\\} \\, , \\quad k \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**Damped (or weighted) Jacobi method**](https://en.wikipedia.org/wiki/Jacobi_method#Weighted_Jacobi_method)\n",
    "\n",
    "$$\n",
    "M = \\omega^{-1}  D =\\omega^{-1} \\textrm{diag}(A)\n",
    "$$\n",
    "\n",
    "with a weight ($\\omega \\ne 0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**Richardson method**](https://en.wikipedia.org/wiki/Modified_Richardson_iteration)\n",
    "\n",
    "$$\n",
    "M = \\omega^{-1} I \n",
    "$$\n",
    "\n",
    "with a weight $(\\omega \\ne 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**Gauss-Seidel method**](https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method)\n",
    "\n",
    "$$\n",
    "M = D + L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Successive over-relaxation (SOR) method](https://en.wikipedia.org/wiki/Successive_over-relaxation)\n",
    "\n",
    "$$\n",
    "M = \\omega^{-1} D + L\n",
    "$$\n",
    "\n",
    "for some weight, a.k.a., over-relaxation factor $(\\omega \\ne 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Symmetric Successive over-relaxation (SSOR) method](https://en.wikipedia.org/wiki/Symmetric_successive_over-relaxation)\n",
    "\n",
    "$$\n",
    "M = \\frac{1}{\\omega (2-\\omega)}( D+\\omega L  ) D^{-1} \\left( D+\\omega U  \\right)\n",
    "$$\n",
    "\n",
    "for some weight, a.k.a., over-relaxation factor ($\\omega \\notin \\{0,2 \\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear stationary iterative methods are also called relaxation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Krylov subspace](https://en.wikipedia.org/wiki/Krylov_subspace) methods:\n",
    "\n",
    "This class of methods are designed to solve a system with a suitable basis, comprised of the order-$r$ Krylov subspace, generated by an n-by-n matrix $A$ and a vector $b$ of dimension $n$. This is defined as the linear subspace spanned by the images of $b$ under the first $r$ powers of A (starting from $A^0 = I$), that is\n",
    "\n",
    "$$\n",
    "\\textrm{span} \\left \\{b, Ab, A^2b, \\dots, A^{r-1}b \\right \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krylov subspace methods try to avoid matrix-matrix operations, but rather multiply vectors by the matrix and work with the resulting vectors. Starting with a vector $b$, one computes $Ab$, then one multiplies that vector by $A$ again to find $A^2 b$ and so on. All algorithms that work this way are referred to as Krylov subspace methods; they are among the most successful methods currently available in numerical linear algebra.\n",
    "\n",
    "These methods can be used in situations where there is an algorithm to compute the matrix-vector multiplication without there being an explicit representation of $A$, giving rise to [**matrix-free methods**](https://en.wikipedia.org/wiki/Matrix-free_methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Conjugate-Gradient method:\n",
    "\n",
    "Another very interesting method, that uses the residual form and can be seen both as a direct method and an iterative method is the [Conjugate-Gradient method](https://en.wikipedia.org/wiki/Conjugate_gradient_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example to solve a Partial Differential Equation\n",
    "\n",
    "A LOT of physical processes and engineering problems can be described via Partial Differential Equations (PDEs). PDEs model phenomenon in many fields with great societal impact such as medicine, geophysics, climate change, electrodynamics, finance, economics, weather forecasting, etc. \n",
    "\n",
    "Approximate solutions for large systems of PDEs can only be found using numerical techniques.\n",
    "\n",
    "Here we introduce an example of a very simple diffusion problem in Physics (heat diffusion), the [**heat (diffusion) equation**](https://en.wikipedia.org/wiki/Heat_equation) in $1$ dimension:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a $1D$ domain $[0,L]$ (can represent a rod), where the temperature function $u=u(x,t)$ varies in time and space.\n",
    "\n",
    "We define uniformly equi-spaced points in the domain $x_i = x_0 + i \\Delta x $, with $x_0=0$ and $x_{m}=L$.\n",
    "\n",
    "Then, at a given point in space and time, we approximate our solution by\n",
    "\n",
    "$$\n",
    "u^n_i \\sim u(t=t^n, x=x_i) = u(t_0 + n \\Delta t, x_0 + i \\Delta x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This define our solution vector, where we consider Dirichelet (homogeneous) boundary conditions: $u_0 = u(x_0) = 0$ and  $u_m = u(x_m) = 0$, for all times $t$\n",
    "\n",
    "$$\n",
    "\n",
    "\\mathbf{u} = \\begin{bmatrix} u_0 = 0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{m-1} \\\\ u_m = 0 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite Differences\n",
    "\n",
    "To define derivatives, we use Taylor's expansion:\n",
    "\n",
    "$$\n",
    "u_{i+1} = u(x_i + \\Delta x) \\approx u(x_i) + \\Delta x \\partial_x u \\bigg\\rvert_i + \\frac{1}{2} \\Delta x^2 \\partial^2_x u \\bigg\\rvert_i + \\frac{1}{6} \\Delta x^3 \\partial^3_x u \\bigg\\rvert_i + O(\\Delta x^4)\n",
    "$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "u_{i-1} = u(x_i - \\Delta x) \\approx u(x_i) - \\Delta x \\partial_x u \\bigg\\rvert_i + \\frac{1}{2} \\Delta x^2 \\partial^2_x u \\bigg\\rvert_i - \\frac{1}{6} \\Delta x^3 \\partial^3_x u \\bigg\\rvert_i + O(\\Delta x^4)\n",
    "$$\n",
    "\n",
    "We can define the first-order derivative at the point $x_i$ using the **forward** difference:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u_i^F}{\\partial x} \\approx \\frac{ u_{i+1} - u_{i}}{\\Delta x}\n",
    "$$\n",
    "\n",
    "Similarly, we can define the first-order derivative at the point $x_i$ using the **backward** difference:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u_{i}^B}{\\partial x} \\approx \\frac{ u_{i} - u_{i-1}}{\\Delta x}\n",
    "$$\n",
    "\n",
    "\n",
    "We can now define a second-order derivative, at the point $x_i$ using a centered difference formula:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 u^C_i}{\\partial x^2} = \\frac{\\frac{\\partial u_{i+\\frac{i}{2}}^F}{\\partial x} -\\frac{ \\partial u_{i-\\frac{i}{2}}^B}{\\partial x} }{\\Delta x} \\approx  \\frac{u_{i-1} -2 u_i + u_{i+1}}{\\Delta x^2} + O(\\Delta x^2)\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this for all points $x_i \\in [0,L]$, we obtain a matrix representation of the second-order finite difference centered operator:\n",
    "\n",
    "$$\n",
    "A = \\frac{1}{\\Delta x^2} \\begin{bmatrix} -2 & 1 & 0 & & 0  \\\\ 1 & -2 & 1 & & \\vdots  \\\\ 0 & 1 & -2 &  \\\\ & & &  \\ddots & 1 \\\\ 0 & \\ldots & & 1 & -2\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We can factor out the $\\frac{1}{\\Delta x^2}$ term in front and from now on we'll consider the matrix of coefficients only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "In Physics, it is customary to write the problem with a negative sign:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = -\\frac{\\partial^2 u}{\\partial x^2}\n",
    "$$\n",
    "\n",
    "This way, your matrix A is SPD.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To solve this with **Jacobi**'s method:\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}  -2 & 0 & 0 & & 0  \\\\ 0 & -2 & 0 & & \\vdots  \\\\ 0 & 0 & -2 &  \\\\ & & &  \\ddots & 0\\\\ 0 & \\ldots & & 0 & -2 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "N = \\begin{bmatrix} 0 & -1 & 0 & & 0  \\\\ -1 & 0 & -1 & & \\vdots  \\\\ 0 & -1 & 0 &  \\\\ & & &  \\ddots & -1\\\\ 0 & \\ldots & & -1 & 0\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, \n",
    "\n",
    "$$\n",
    "M^{-1}N =  \\begin{bmatrix} 0 & 1/2 & 0 & & 0  \\\\ 1/2 & 0 & 1/2 & & \\vdots  \\\\ 0 & 1/2 & 0 &  \\\\ & & &  \\ddots & 1/2 \\\\ 0 & \\ldots & & 1/2 & 0\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a spectral radius, $\\rho (M^{-1}N ) = \\cos\\left(\\frac{\\pi}{m+1} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To solve this with **Gauss-Seidel**'s method:\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}  -2 & 0 & 0 & & 0  \\\\ 1 & -2 & 0 & & \\vdots  \\\\ 0 & 1 & -2 &  \\\\ & & &  \\ddots & 0\\\\ 0 & \\ldots & & 1 & -2 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "N = \\begin{bmatrix} 0 & -1 & 0 & & 0  \\\\ 0 & 0 & -1 & & \\vdots  \\\\ 0 & 0 & 0 &  \\\\ & & &  \\ddots & -1\\\\ 0 & \\ldots & & 0 & 0 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in this case the spectral radius, $\\rho (M^{-1}N ) = \\cos^2\\left(\\frac{\\pi}{m+1} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To solve this with **SOR**'s method:\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}  -2 & 0 & 0 & & 0  \\\\ \\omega & -2 & 0 & & \\vdots  \\\\ 0 & \\omega & -2 &  \\\\ & & &  \\ddots & 0\\\\ 0 & \\ldots & & \\omega & -2 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "N = \\begin{bmatrix} (\\omega-1) & -\\omega & 0 & & 0  \\\\ 0 & (\\omega-1) & -\\omega & & \\vdots  \\\\ 0 & 0 & 0 &  \\\\ & & &  \\ddots & -\\omega\\\\ 0 & \\ldots & & 0 & (\\omega-1) \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which has a spectral radius, $\\rho (M^{-1}N ) = \\frac{ \\cos^2\\left(\\frac{\\pi}{m+1} \\right)}{\\left( 1+\\sin\\left( \\frac{\\pi}{m+1} \\right) \\right)^2}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
